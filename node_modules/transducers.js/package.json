{
  "name": "transducers.js",
  "version": "0.2.3",
  "author": {
    "name": "James Long",
    "email": "longster@gmail.com"
  },
  "main": "./transducers.js",
  "repository": {
    "type": "git",
    "url": "https://github.com/jlongster/transducers.js.git"
  },
  "keywords": [
    "browser",
    "client",
    "functional",
    "util"
  ],
  "devDependencies": {
    "benchmark": "^1.0.0",
    "es6-macros": "0.0.7",
    "expect.js": "^0.3.1",
    "gulp": "^3.8.8",
    "gulp-header": "^1.1.1",
    "gulp-sourcemaps": "^1.1.5",
    "gulp-sweetjs": "^0.5.4",
    "immutable": "^2.0.17",
    "lodash": "git://github.com/lodash/lodash",
    "mocha": "^1.21.4",
    "source-map-support": "^0.2.7",
    "sweet.js": "^0.7.1",
    "underscore": "^1.7.0",
    "webpack": "^1.4.0-beta9"
  },
  "readme": "\n# transducers.js\n\nA small library for generalized transformation of data. This provides a bunch of transformation functions that can be applied to any data structure. It is a direct port of Clojure's [transducers](http://blog.cognitect.com/blog/2014/8/6/transducers-are-coming) in JavaScript. Read more in [this post](http://jlongster.com/Transducers.js--A-JavaScript-Library-for-Transformation-of-Data).\n\nThe algorithm behind this, explained in the above post, not only allows for it to work with any data structure (arrays, objects, iterators, immutable data structures, you name it) but it also provides better performance than other alternatives such as underscore or lodash. This is because there are no intermediate collections. See [this post](http://jlongster.com/Transducers.js-Round-2-with-Benchmarks) for benchmarks.\n\n```\nnpm install transducers.js\n```\n\nFor browsers, grab the file `dist/transducers.js`.\n\nWhen writing programs, we frequently write methods that take in collections, do something with them, and return a result. The problem is that we frequently only write these functions to work a specific data structure, so if we ever change our data type or wanted to reuse that functionality, you can't. We need to decouple these kinds of concerns.\n\nA transducer is a function that takes a reducing function and returns a new one. It can perform the necessary work and call the original reducing function to move on to the next \"step\". In this library, a transducer a little more than that (it's actually an object that also supports init and finalizer methods) but generally you don't have to worry about these internal details. Read [my post](http://jlongster.com/Transducers.js--A-JavaScript-Library-for-Transformation-of-Data) if you want to learn more about the algorithm.\n\n```js\nvar transform = compose(\n  map(x => x * 3),\n  filter(x => x % 2 === 0),\n  take(2)\n);\n\nseq([1, 2, 3, 4, 5], transform);\n// -> [ 6, 12 ]\n\nfunction* nums() {\n  var i = 1;\n  while(true) {\n    yield i++;\n  }\n}\n\ninto([], transform, nums());\n// -> [ 6, 12 ]\n\ninto([], transform, Immutable.Vector(1, 2, 3, 4, 5))\n// -> [ 6, 12 ]\n```\n\nAll of these work with arrays, objects, and any iterable data structure (like [immutable-js](https://github.com/facebook/immutable-js)) and you get all the high performance guarantees for free. The above code always only performs 2 transformations because of `take(2)`, no matter how large the array. This is done without laziness or any overhead of intermediate structures.\n\n## Transformations\n\nThe following transformations are available, and there are more to come (like `partition`).\n\n* `map(coll?, f, ctx?)` &mdash; call `f` on each item\n* `filter(coll?, f, ctx?)` &mdash; only include the items where the result of calling `f` with the item is truthy\n* `remove(coll?, f, ctx?)` &mdash; only include the items where the result of calling `f` with the item is falsy\n* `keep(coll?)` &mdash; remove all items that are `null` or `undefined`\n* `take(coll?, n)` &mdash; grab only the first `n` items\n* `takeWhile(coll?, f, ctx?)` &mdash; grab only the first items where the result of calling `f` with the item is truthy\n* `drop(coll?, n)` &mdash; drop the first `n` items and only include the rest\n* `dropWhile(coll?, f, ctx?)` &mdash; drop the first items where the result of calling `f` with the item is truthy\n* `dedupe(coll?)` &mdash; remove consecutive duplicates (equality compared with ===)\n\nThe above functions optionally take a collection to immediately perform the transformation on, and a context to bind `this` to when calling `f`. That means you can call them in four ways:\n\n* Immediately perform a map: `map([1, 2, 3], x => x + 1)`\n* Same as above but call the function with `this` as `ctx`: `map([1, 2, 3], function(x) { return x + 1; }, ctx)`\n* Make a map transducer: `map(x => x + 1)`\n* Same as above but with `this` as `ctx`: `map(function(x) { return x + 1; }, ctx)`\n\n(I will be using the ES6 fat arrow syntax, but if that's not available just `function` instead)\n\nThe signature of running an immediate map is the same familiar one as seen in lodash and underscore, but now you can drop the collection to make a transducer and run multiple transformations with good performance:\n\n```js\nvar transform = compose(\n  map(x => x + 1),\n  filter(x => x % 2 === 0),\n  take(2)\n);\n```\n\n`compose` is a provided function that simply turns `compose(f, g)` into `x => f(g(x))`. You use it to build up transformations. The above transformation would always run the map and filter **only twice** becaue only two items are needed, and it short-circuits once it gets two items. Again, this is done without laziness, read more [here](http://jlongster.com/Transducers.js--A-JavaScript-Library-for-Transformation-of-Data).\n\nThere are also 2 transducers available for taking collections and \"catting\" them into the transformation stream:\n\n* `cat` &mdash; take collections and forward each item individually, essentially flattening it\n* `mapcat(f)` &mdash; same as `cat`, but first apply `f` to each collection\n\nJust pass `cat` straight through like so: `compose(filter(x => x.length < 10), cat)`. That would take all arrays with a length less than 10 and flatten them out into a single array.\n\n## Applying Transformations\n\nBuilding data structure-agnostic transformations is cool, but how do you actually use them? `transducers.js` provides several integration points.\n\nTo use a transformation, we need to know how to iterate over the source data structure and how to build up a new one. The former is easy; we can work with arrays, objects, and anything can uses the ES6 iterator protocol (Maps, Sets, generators, etc). All the the below functions works with them.\n\nFor the latter, you need to specify what you want back. The following functions allow you to make a new data structure and possibly apply a transformation:\n\n* `toArray(coll, xform?)` &mdash; Turn `coll` into an array, applying the transformation `xform` to each item if provided. The transform is optional in case you want to do something like turn an iterator into an array.\n* `toObj(coll, xform?)` &mdash; Turn `coll` into an object if possible, applying the transformation `xform` if provided. When an object is iterated it produces two-element arrays `[key, value]`, and `obj` will turn these back into an object.\n* `toIter(coll, xform?)` &mdash; Make an iterator over `coll`, and apply the transformation `xform` to each value if specified. Note that `coll` can just be another iterator. **Transformations will be applied lazily**.\n* `seq(coll, xform)` &mdash; A generalized method that will return the same data type that was passed in as `coll`, with `xform` applied. You will usually use this unless you know you want an array, object, or iterator. If `coll` is an iterator, another iterator will be returned and transformations will be applied lazily.\n* `into(to, xform, from)` &mdash; Apply `xform` to each item in `from` and append it to `to`. This has the effect of \"pouring\" elements into `to`. You will commonly use this when converting one type of object to another.\n* `transduce(coll, xform, reducer, init?)` &mdash; Like `reduce`, but apply `xform` to each value before passing to `reducer`. If `init` is not specify it will attempt to get it from `reducer`.\n\nThe possibilities are endless:\n\n```js\n// Map an object\nseq({ foo: 1, bar: 2 }, map(kv => [kv[0], kv[1] + 1]));\n// -> { foo: 2, bar: 3 }\n\n// Make an array from an object\ntoArray({ foo: 1, bar: 2 });\n// -> [ [ 'foo', 1 ], [ 'bar', 2 ] ]\n\n// Make an array from an iterable\nfunction* nums() {\n  var i = 1;\n  while(true) {\n    yield i++;\n  }\n}\ninto([], take(3), nums());\n// -> [ 1, 2, 3 ]\n\n// Lazily transform an iterable\nvar iter = seq(nums(), compose(map(x => x * 2),\n                               filter(x => x > 4));\niter.next().value; // -> 6\niter.next().value; // -> 8\niter.next().value; // -> 10\n```\n\n## Laziness\n\nTransducers remove the requirement of being lazy to optimize for things like `take(10)`. However, it can still be useful to \"bind\" a collection to a set of transformations and pass it around, without actually evaluating the transformations.\n\nAs noted above, whenever you apply transformations to an iterator it does so lazily. It's easy convert array transformations into a lazy operation, just use the utility function `iterator` to grab an iterator of the array instead:\n\n```js\nseq(iterator([1, 2, 3]),\n    compose(\n      map(x => x + 1),\n      filter(x => x % 2 === 0)))\n// -> <Iterator>\n```\n\nOur transformations are completely blind to the fact that our transformations may or may not be lazy.\n\n## Utility Functions\n\nThis library provides a few small utility functions:\n\n* `iterator(coll)` &mdash; Get an iterator for `coll`, which can be any type like array, object, iterator, or custom data type\n* `push(arr, value)` &mdash; Push `value` onto `arr` and return `arr`\n* `merge(obj, value)` &mdash; Merge `value` into `obj`. `value` can be another object or a two-element array of `[key, value]`\n* `range(n)` &mdash; Make an array of size `n` filled with numbers from `0..n`.\n\n## immutable-js\n\nWe've talked about how this can be applied to any data structure &mdash; let's see that in action. Here's how you could use this with [immutable-js](https://github.com/facebook/immutable-js).\n\n```js\nImmutable.Vector.from(\n  seq(Immutable.Vector(1, 2, 3, 4, 5),\n      compose(\n        map(function(x) { return x + 10; }),\n        map(function(x) { return x * 2; }),\n        filter(function(x) { return x % 5 === 0; }),\n        filter(function(x) { return x % 2 === 0; })))\n)\n```\n\nWe can use our familiar `seq` function because `Immutable.Vector` implements the iterator protocol, so we can iterator over it. Because `seq` is working with an iterator, it returns a new iterator that will *lazily transform each value*. We can simply pass this iterator into `Immutable.Vector.from` to construct a new one, and we have a new transformed immutable vector with no intermediate collections except for one lazy transformer!\n\nThe builtin transformations perform well because they minimize allocations, but since we don't have any intermediate structures or laziness machinery, this performs slightly better. The point is not to beat it, but to show that both are high-performance but we can apply our performance to any data structure.\n\n## CSP Channels\n\nThis not only works with all the JavaScript data structures you can think of, but it even works for things like streams. Soon channels from [js-csp](https://github.com/ubolonton/js-csp) will be able to take a transformation and you get all of this for channels for free:\n\n```js\nvar ch = chan(1, compose(\n  cat,\n  map(x => x + 1),\n  dedupe(),\n  drop(3)\n));\n```\n\n## The `transformer` protocol\n\nWhile it's great that you can apply transducers to custom data structures, it's a bit annoying to always have to use constructor functions like `Immutable.Vector.from`. One option is to define a new protocol complementary to `iterator`. I call it the `transformer` protocol.\n\nTo implement the transformer protocol, you add a transformer to the prototype of your data structure. A transformer is an object with three methods: `init`, `result`, and `step`. `init` returns a new empty object, `result`, can perform any finalization steps on the resulting collection, and `step` perform a reduce. Here's what it looks like for `Immutable.Vector`:\n\n```js\nvar t = require('./transducers');\nImmutable.Vector.prototype[t.protocols.transformer] = {\n  init: function() {\n    return Immutable.Vector().asMutable();\n  },\n  result: function(vec) {\n    return vec.asImmutable();\n  },\n  step: function(vec, x) {\n    return vec.push(x);\n  }\n};\n```\n\nIf you implement the transformer protocol, now your data structure will work with *all* of the builtin functions. You can just use `seq` like normal and you get back an immutable vector!\n\n```js\nt.seq(Immutable.Vector(1, 2, 3, 4, 5),\n      t.compose(\n        t.map(function(x) { return x + 10; }),\n        t.map(function(x) { return x * 2; }),\n        t.filter(function(x) { return x % 5 === 0; }),\n        t.filter(function(x) { return x % 2 === 0; })));\n// -> Vector [ 30 ]\n```\n\n## Running Tests\n\n```\nnpm install\ngulp\nmocha build/tests\n```\n\n[BSD LICENSE](https://github.com/jlongster/transducers.js/blob/master/LICENSE)\n",
  "readmeFilename": "README.md",
  "description": "A small library for generalized transformation of data. This provides a bunch of transformation functions that can be applied to any data structure. It is a direct port of Clojure's [transducers](http://blog.cognitect.com/blog/2014/8/6/transducers-are-coming) in JavaScript. Read more in [this post](http://jlongster.com/Transducers.js--A-JavaScript-Library-for-Transformation-of-Data).",
  "bugs": {
    "url": "https://github.com/jlongster/transducers.js/issues"
  },
  "homepage": "https://github.com/jlongster/transducers.js",
  "_id": "transducers.js@0.2.3",
  "dist": {
    "shasum": "1bcd09e6a355aff0afe76231ef2b9fd5d17d745c"
  },
  "_from": "transducers.js@",
  "_resolved": "https://registry.npmjs.org/transducers.js/-/transducers.js-0.2.3.tgz"
}
